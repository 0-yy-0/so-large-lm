## 智能体(Agent)

# 简介

基于大型语言模型（LLM）的智能体，以下简称LLM智能体，指的是能够执行复杂任务的LLM应用，通过结合LLM与规划、记忆等关键模块的架构来实现。在构建LLM智能体时，LLM作为主控制器或“大脑”，控制完成任务或用户请求所需的操作流程。LLM智能体可能需要诸如规划、记忆和工具使用等关键模块。

为了更好地说明LLM智能体的用处，假设我们想构建一个系统，以回答如下问题：

> 2023年中国的平均月收入是多少？
> 

上述问题可以通过一个已经具备回答该问题所需知识的LLM直接回答。如果LLM没有相关知识来回答这个问题，可以使用一个简单的RAG（检索增强生成）系统，其中LLM可以访问与健康相关的信息或报告。现在，让我们来看一个更复杂的问题：

> 过去十年中，中国成年人的平均月收入的趋势如何变化，这对国民消费有什么影响？此外，你能提供这一时期消费趋势的图形表示吗？
> 

要回答这样一个问题，仅使用LLM是不够的。你可以将LLM与外部知识库结合形成RAG系统，但这仍然可能不足以回答上述复杂的查询。这是因为上述复杂问题需要LLM将任务分解为子部分，这些子部分可以使用工具和操作流程来解决，从而得到最终所需的答案。一个可能的解决方案是构建一个LLM智能体，它可以访问搜索API、健康相关出版物以及公共/私人健康数据库，以提供与卡路里摄入和肥胖相关的信息。

此外，LLM还需要访问一个“代码解释器”工具，帮助利用相关数据生成有助于理解肥胖趋势的有用图表。这些可能是假设LLM智能体的高级组件，但还有一些重要的考虑因素，如创建解决任务的计划，以及可能访问记忆模块，帮助智能体跟踪操作流程、观察和整体进度的状态。

# LLM Agent 架构

一般而言，基于LLM的智能体框架包括以下核心组件：

- 用户请求 - 用户的问题或请求
- 智能体/大脑 - 充当协调者的智能体核心
- 规划 - 协助智能体规划未来的行动
- 记忆 - 管理智能体的过往行为
    
![](images/agent.png)
    

## 智能体

具有通用能力的大语言模型（LLM）作为系统的主要大脑、智能体模块或多个任务的协调者。该组件将通过直接执行操作和将访问的工具（外部工具/大模型自身内部）使其执行指令（指令主要是提示模板prompt）。

虽然智能体的设定不是强制性的，但可以对智能体进行概况描述或赋予其特定人格来定义其角色。这些概况信息通常写在提示中，可以包括角色细节、个性、社会信息和其他人口统计信息。

## 规划

### 无反馈规划

规划模块帮助分解智能体将单独解决的必要步骤或子任务，以回答用户请求。这一步骤对于使智能体更好地理解问题并可靠地找到解决方案非常重要。规划模块将利用LLM分解详细计划，包括帮助解决用户问题的子任务。任务分解的流行技术包括[思维链（COT](https://github.com/amazon-science/auto-cot)）和[思维树（TOT）](https://github.com/princeton-nlp/tree-of-thought-llm)，分别可以归类为单路径推理和多路径推理。

![](images/tot.png)

### 有反馈规划

上述规划模块不涉及任何反馈，这使得实现解决复杂任务的长期规划变得具有挑战性。为了解决这一挑战，可以利用一种机制，使模型能够根据过去的行动和观察反复思考和细化执行计划。目标是纠正并改进过去的错误，这有助于提高最终结果的质量。这在复杂的现实世界环境和任务中尤其重要，其中试错是完成任务的关键。这种反思或批评机制的两种流行方法包括[ReAct](https://github.com/ysymyth/ReAct)和[Reflexion](https://github.com/noahshinn/reflexion)。

![](images/act.png)

## 记忆

记忆模块帮助存储智能体的内部日志，包括过去的思考、行动和来自环境的观察，以及智能体与用户之间的所有互动。LLM智能体文献报告了两种主要的记忆类型：

- 短期记忆 - 包括有关智能体当前情况的上下文信息；这通常通过上下文学习实现，意味着由于上下文窗口限制，它是短暂且有限的。
- 长期记忆 - 包括需要在较长时间内保留和回忆的智能体过去的行为和思考；这通常利用通过快速可扩展检索访问的外部向量存储，以根据需要为智能体提供相关信息。
- 混合记忆集成了短期记忆和长期记忆，以提高智能体的长期推理能力和经验积累。

构建智能体时还需要考虑不同的记忆格式。代表性的记忆格式包括自然语言、嵌入、数据库和结构化列表等。规划和记忆模块使智能体能够在动态环境中运作，并使其能够有效地回忆过去的行为和规划未来的行动。

## 工具

工具对应于一组使LLM智能体能够与外部环境（如Wikipedia搜索API、代码解释器和数学引擎）交互的工具/工具集。工具还可以包括数据库、知识库和外部模型。当智能体与外部工具交互时，它通过协助智能体获得观察或完成子任务所需的信息的工作流来执行任务。在我们最初的与健康相关的查询中，代码解释器是一个工具的例子，它执行代码并生成用户请求的必要图表信息。

LLM以不同方式利用工具：

- [MRKL](https://arxiv.org/abs/2205.00445)：是一个将LLM与专家模块（LLM或符号化，如计算器或天气API）结合的框架。
- [Toolformer](https://arxiv.org/abs/2302.04761)：对LLM进行微调，以使用外部工具API。
- 函数调用（Function Calling）： 增强LLM的工具使用能力，涉及定义一组工具API，并将其作为请求的一部分提供给模型。
- [HuggingGPT](https://github.com/microsoft/JARVIS) ： 一个由LLM驱动的智能体，利用LLM作为任务规划器，连接各种现有的AI模型（基于描述）来解决AI任务。

![](images/tool.png)

# Agent的挑战：

构建基于LLM的智能体仍处于起步阶段，存在许多挑战和限制：

- 角色扮演能力：基于LLM的智能体通常需要适应一个角色，以有效地完成特定领域的任务。对于LLM表征或迁移不好的角色，可以通过在代表非常见角色或心理特征的数据上微调LLM来解决。
- 长期规划和有限的上下文长度：跨越长期历史的规划仍然是一个挑战，可能导致智能体出错而无法恢复。LLM支持的上下文长度也有限，这可能导致限制智能体能力的约束，如利用短期记忆。
- 通用人类价值对齐：将智能体与多样化的人类价值对齐也是一个挑战，这在标准LLM中也很常见。一个潜在的解决方案包括通过设计先进的提示策略重新对齐LLM的潜力。
- 提示的鲁棒性和可靠性：LLM智能体可能涉及设计用于驱动不同模块（如记忆和规划）的多个提示。即使是最微小的提示变化，也常见于LLM中的可靠性问题。LLM智能体涉及整个提示框架，这使其更容易出现鲁棒性问题。潜在的解决方案包括通过试错、自动优化/调整提示或使用GPT自动生成提示来设计提示元素。LLM的另一个常见问题是幻觉，这在LLM智能体中也很普遍。这些智能体依赖自然语言与外部组件交互，可能引入冲突信息，导致幻觉和事实性问题。
- 知识边界：与可能导致幻觉或事实性问题的知识不匹配问题类似，控制LLM的知识范围是一个挑战，这可以显著影响模拟的有效性。具体而言，LLM的内部知识可能引入偏见或使用用户不知道的知识，这可能影响智能体在特定环境中的行为。
- 效率：LLM智能体涉及大量由LLM处理的请求，这可能影响智能体行为的效率，因为它将严重依赖于LLM的推理速度。部署多个智能体时，成本也是一个关注点。
