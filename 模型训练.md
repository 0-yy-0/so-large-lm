# 第四章 模型训练

上一章中，我们讨论了大语言模型（例如Transformer）的模型结构。
在本章中，我们将讨论如何训练大语言模型。
本章分成目标函数和优化算法两部分。

# 4.1 目标函数
这里，我们研究三类语言模型的目标函数：
1. 只包含解码器（Decoder-only）的模型（例如，GPT-3）：计算单向上下文嵌入（contextual embeddings），一次生成一个token
2. 只包含编码器（Encoder-only）的模型（例如，BERT）：计算双向上下文嵌入
3. 编码器解码器（Encoder-decoder）模型（例如T5）：编码输入，解码输出

我们可以使用任何模型将token序列映射到上下文嵌入中（例如，LSTM、Transformers）：

$$
\phi : V^L \to \R^{d \times L}. \\
\left[\text{the}, \text{mouse}, \text{ate}, \text{the}, \text{cheese}\right] \to \left[\binom{1}{0.1}, \binom{0}{1}, \binom{1}{1}, \binom{1}{-0.1}, \binom{0}{-1} \right].
$$

## 4.1.1 Decoder-only 模型


回想一下，自回归语言模型定义了一个条件分布：
$$
p(x_i \mid x_{1:i-1}).
$$
我们将其定义如下：
1. 将$x_{1:i-1}$映射到上下文嵌入$\phi(x_{1:i-1})$。
2. 应用嵌入矩阵$E \in \R^{V \times d}$来获得每个token的得分$E \phi(x_{1:i-1})_{i-1}$。
3. 对其进行指数化和归一化以产生$x_i$上的分布。

简洁地：
$$
p(x_{i+1} \mid x_{1:i}) = softmax(E \phi(x_{1:i})_i).
$$

另外，这里引入**最大似然**的概念。
设$\theta$是大型语言模型的所有参数。设$D$是由一组序列组成的训练数据。
然后，我们可以遵循最大似然原理，定义以下负对数似然目标函数：
$$
O(\theta) = \sum_{x \in D} - \log p_\theta(x) = \sum_{x \in D} \sum_{i=1}^L -\log p_\theta(x_i \mid x_{1:i-1}).
$$

关于如何有效地优化这一目标函数，还有更多的方法，但这就是目标函数的全部。

# 4.2 优化算法
现在我们将注意力转向如何优化目标函数。
为了简单起见，让我们以自回归语言模型为例：
$$
O(\theta) = \sum_{x \in D} -\log p_\theta(x).
$$

## 4.2.1 随机梯度下降（SGD）
最简单的优化算法是用小批量进行随机梯度下降，该算法的步骤如下：
- 初始化参数$\theta_0$
- 重复：
    - 采样小批量$B_t \subset D$
    - 根据梯度更新参数：
    $$
    \theta_t \leftarrow \theta_{t-1} - \eta \frac{1}{|B_t|} \sum_{x \in B_t} \nabla_\theta (-\log p_\theta(x)).
    $$

优化的关键点包括：
1. 我们希望参数$\theta$可以快速收敛
2. 我们希望优化在数值上是稳定的
3. 我们希望内存高效（尤其是对于大模型）

这些点往往相互矛盾（例如，通过低精度训练，可以实现快速收敛、减少内存占用，但是会导致训练不稳定）

因此，我们可以从几个层次来进行优化：
1. 针对经典优化：二阶方法、约束优化等。
2. 针对机器学习：随机方法、隐式正则化+早停法
3. 针对深度学习：初始化、归一化（更改模型架构）
4. 针对大语言模型：由于稳定性问题，学习率和一些直觉（例如，二阶方法）仍然有用，但要使大语言模型有效训练，还需要克服许多其他独特的挑战。不幸的是，其中大部分内容都是特别的，人们对此了解甚少。



## 4.2.2 Adam
[Adam](https://arxiv.org/pdf/1412.6980.pdf)(adaptive moment estimation, 自适应动量估计)算法拥有以下两个创新：
1. 引入动量（继续朝同一方向移动）。
2. 参数$\theta_0$的每个维度都有一个自适应（不同）的步长（受二阶方法启发）。

它的步骤如下：
- 初始化参数$\theta_0$
- 初始化动量$m_0, v_0 \leftarrow 0$
- 重复：
    - 采样小批量$B_t \subset D$
    - 按照如下步骤更新参数：
        - 计算梯度
        $$
        g_t \leftarrow \frac{1}{|B_t|} \sum_{x \in B_t} \nabla_\theta (-\log p_\theta(x)).
        $$

        - 更新一阶、二阶动量
        $$
        m_t \leftarrow \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
        v_t \leftarrow \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
        $$

        - 对偏差进行修正
        $$
        \hat m_t \leftarrow m_t / (1 - \beta_1^t) \\
        \hat v_t \leftarrow v_t / (1 - \beta_2^t)
        $$

        - 更新参数
        $$
        \theta_t \leftarrow \theta_{t-1} - \eta \, \hat m_t / (\sqrt{\hat v_t} + \epsilon).
        $$

存储占用分析：

Adam将存储从2倍的模型参数（$\theta_t,g_t$）增加到了4倍（$\theta_t,g_t,m_t,v_t$）。

## 4.2.3 AdaFactor
[AdaFactor](https://arxiv.org/pdf/1804.04235.pdf)是一种为减少存储占用的优化算法。它有如下特点：
- 它不储存$m_t,v_t$这样的$O(m \times n)$矩阵，而是存储行和列的和$O(m + n)$并重构矩阵
- 去除动量
- 它被用来训练T5
- AdaFactor可能使训练变得困难（见[Twitter thread](https://twitter.com/_arohan_/status/1468673364889726985?s=20&amp;t=i7E0NN5ytysukMGVWG7lfQ)和[blog post](https://blog.ceshine.net/post/adafactor/)）

## 4.2.4 混合精度训练

[混合精度训练](https://arxiv.org/pdf/1710.03740.pdf)是另一种减少存储的方法
- 通常来说，默认的精度是：FP32（32位浮点）
- 其他可选精度：FP16（16位浮点），但问题是任何小于$2^{-24}$的值都会变为0。
- 解决方案：将主权重存储在FP32中，并在FP16中执行其他所有操作。
- 损失缩放：按比例放大损失，以避免梯度数值太小。
- 结果：存储减少了一半。

![混合精度训练](./images/mixed-precision-training.png)

## 4.2.5 学习率
通常情况下，学习率会随着时间的推移而降低。
对于Transformer模型，我们实际上需要通过预热（warmup）提高学习率。
[Huang等人，2020](https://www.cs.toronto.edu/~mvolkovs/ICML22020_tfixup.pdf)表明，一个潜在的原因是防止层归一化的梯度消失，导致使用Adam优化器训练时不稳定。

## 4.2.6 初始化
- 给定矩阵$(W \in \R^{m \times n}$，标准初始化（即xavier初始化）为$W_{ij} \sim N(0, 1/n)$，其中\（n\）是扇进来的。
- GPT-2和GPT-3通过额外的$1/\sqrt{N}$缩放权重，其中$N$是残差层的数量。
- T5将注意力矩阵增加一个$1/\sqrt{d}$[代码](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/attention.py#L459)

以GPT-3为例，使用的参数如下：
- Adam参数：$\beta_1 = 0.9, \beta_2 = 0.95, \epsilon = 10^{-8}$
- 批量大小：320万个token（约1500个序列）
- 使用梯度剪裁（$g_t \leftarrow g_t / \min(1, \|g\|_2)$）
- 线性学习率预热（前3.75亿个token）
- [余弦学习率](https://arxiv.org/pdf/1608.03983v5.pdf)衰减到10%
- 逐渐增加批量大小
- 权重衰减设为0.1

# 延伸阅读
- [混合精度训练](https://lilianweng.github.io/lil-log/2021/09/24/train-large-neural-networks.html#mixed-precision-training)
- [Fixing Weight Decay Regularization in Adam](https://arxiv.org/pdf/1711.05101.pdf). I. Loshchilov, F. Hutter. 2017. 介绍了AdamW
- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/pdf/2003.10555.pdf). Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. ICLR 2020.
- [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/pdf/2006.03654.pdf). Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. ICLR 2020.
